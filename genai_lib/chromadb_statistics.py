#!/usr/bin/env python3
"""
ChromaDB Statistics Module
==========================
Modul zur Erstellung detaillierter Statistiken fÃ¼r ChromaDB-Collections.
Stellt direkt einsetzbare Funktionen fÃ¼r die Analyse bereit.
"""

import chromadb
import os
from collections import defaultdict, Counter
from typing import Dict, List, Optional, Tuple, Union
import json
from datetime import datetime

# --- Datenklassen fÃ¼r Statistiken ---
class CollectionStats:
    """Datenklasse fÃ¼r Collection-Statistiken."""
    def __init__(self, name: str, chunk_count: int, document_count: int, source_stats: Dict[str, int], 
                 metadata_keys: List[str], avg_chunk_size: float = 0.0):
        self.name = name
        self.chunk_count = chunk_count           # Anzahl der Chunks (EintrÃ¤ge in ChromaDB)
        self.document_count = document_count     # Anzahl der ursprÃ¼nglichen Dokumente/Dateien
        self.source_stats = source_stats         # Chunks pro Quelldatei
        self.metadata_keys = metadata_keys       # VerfÃ¼gbare Metadaten-Felder
        self.avg_chunk_size = avg_chunk_size     # Durchschnittliche Chunk-GrÃ¶ÃŸe in Zeichen
    
    def get_chunks_per_document(self) -> float:
        """Berechnet durchschnittliche Chunks pro Dokument."""
        return self.chunk_count / self.document_count if self.document_count > 0 else 0.0
    
    def get_source_list(self) -> List[str]:
        """Gibt Liste aller Quelldateien zurÃ¼ck."""
        return list(self.source_stats.keys())
    
    def get_largest_source(self) -> Tuple[str, int]:
        """Gibt die Quelle mit den meisten Chunks zurÃ¼ck."""
        if not self.source_stats:
            return ("", 0)
        return max(self.source_stats.items(), key=lambda x: x[1])
    
    def to_dict(self) -> Dict:
        """Konvertiert zu Dictionary fÃ¼r Export."""
        return {
            "name": self.name,
            "chunk_count": self.chunk_count,
            "document_count": self.document_count,
            "chunks_per_document": self.get_chunks_per_document(),
            "average_chunk_size": self.avg_chunk_size,
            "metadata_keys": self.metadata_keys,
            "source_statistics": self.source_stats
        }

class DatabaseStats:
    """Datenklasse fÃ¼r gesamte Datenbankstatistiken."""
    def __init__(self, collections: List[CollectionStats], total_chunks: int, total_documents: int):
        self.collections = collections
        self.total_chunks = total_chunks         # Gesamtanzahl aller Chunks
        self.total_documents = total_documents   # Gesamtanzahl aller ursprÃ¼nglichen Dokumente
        self.collection_count = len(collections)
    
    def get_avg_chunks_per_document(self) -> float:
        """Berechnet durchschnittliche Chunks pro Dokument Ã¼ber alle Collections."""
        return self.total_chunks / self.total_documents if self.total_documents > 0 else 0.0
    
    def get_collection_by_name(self, name: str) -> Optional[CollectionStats]:
        """Gibt Collection-Statistiken fÃ¼r bestimmten Namen zurÃ¼ck."""
        for collection in self.collections:
            if collection.name == name:
                return collection
        return None
    
    def get_largest_collection(self) -> Optional[CollectionStats]:
        """Gibt die Collection mit den meisten Chunks zurÃ¼ck."""
        if not self.collections:
            return None
        return max(self.collections, key=lambda x: x.chunk_count)

# --- Kernfunktionen ---
def analyze_collection(collection_name: str, db_path: str) -> Optional[CollectionStats]:
    """
    Analysiert eine einzelne Collection und erstellt Statistiken.
    
    Args:
        collection_name: Name der zu analysierenden Collection
        db_path: Pfad zur ChromaDB
    
    Returns:
        CollectionStats-Objekt mit Analyseergebnissen oder None bei Fehler
    """
    if not os.path.exists(db_path):
        print(f"âŒ Fehler: Der ChromaDB-Pfad '{db_path}' existiert nicht.")
        return None
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(collection_name)
    except Exception as e:
        print(f"âŒ Fehler beim Abrufen der Collection '{collection_name}': {type(e).__name__}: {e}")
        return None
    
    # Grundlegende Anzahl
    chunk_count = collection.count()
    
    if chunk_count == 0:
        return CollectionStats(
            name=collection.name,
            chunk_count=0,
            document_count=0,
            source_stats={},
            metadata_keys=[],
            avg_chunk_size=0.0
        )
    
    # Alle Daten abrufen fÃ¼r detaillierte Analyse
    try:
        results = collection.get(
            ids=None, 
            where=None, 
            include=['metadatas', 'documents']
        )
        
        metadatas = results.get('metadatas', [])
        documents = results.get('documents', [])
        
    except Exception as e:
        print(f"âš ï¸ Warnung: Konnte Details fÃ¼r Collection '{collection.name}' nicht abrufen: {e}")
        metadatas = []
        documents = []
    
    # Quellen-Statistiken und DokumentenzÃ¤hlung
    source_chunk_counts = defaultdict(int)
    all_metadata_keys = set()
    unique_sources = set()  # FÃ¼r die ZÃ¤hlung der ursprÃ¼nglichen Dokumente
    
    for metadata in metadatas:
        if metadata:
            # Sammle alle verfÃ¼gbaren Metadaten-SchlÃ¼ssel
            all_metadata_keys.update(metadata.keys())
            
            # ZÃ¤hle Chunks pro Quelle
            source = metadata.get('source', 'Unbekannte Quelle')
            source_chunk_counts[source] += 1
            unique_sources.add(source)  # Eindeutige Quellen sammeln
    
    # Durchschnittliche Chunk-GrÃ¶ÃŸe berechnen
    avg_chunk_size = 0.0
    if documents:
        total_chars = sum(len(doc) if doc else 0 for doc in documents)
        avg_chunk_size = total_chars / len(documents)
    
    # DokumentenzÃ¤hlung: Anzahl eindeutiger Quellen
    document_count = len(unique_sources) if unique_sources else 0
    
    return CollectionStats(
        name=collection.name,
        chunk_count=chunk_count,
        document_count=document_count,
        source_stats=dict(source_chunk_counts),
        metadata_keys=sorted(list(all_metadata_keys)),
        avg_chunk_size=avg_chunk_size
    )

def get_database_statistics(db_path: str) -> Optional[DatabaseStats]:
    """
    Erstellt umfassende Statistiken fÃ¼r die gesamte ChromaDB.
    
    Args:
        db_path: Pfad zur ChromaDB
    
    Returns:
        DatabaseStats-Objekt oder None bei Fehler
    """
    if not os.path.exists(db_path):
        print(f"âŒ Fehler: Der ChromaDB-Pfad '{db_path}' existiert nicht.")
        return None
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collections = client.list_collections()
        collection_stats = []
        total_chunks = 0
        total_documents = 0
        
        for collection in collections:
            stats = analyze_collection(collection.name, db_path)
            if stats:
                collection_stats.append(stats)
                total_chunks += stats.chunk_count
                total_documents += stats.document_count
        
        return DatabaseStats(
            collections=collection_stats,
            total_chunks=total_chunks,
            total_documents=total_documents
        )
        
    except Exception as e:
        print(f"âŒ Fehler beim Analysieren der Datenbank: {type(e).__name__}: {e}")
        return None

def list_collections(db_path: str) -> List[str]:
    """
    Listet alle verfÃ¼gbaren Collections in der Datenbank auf.
    
    Args:
        db_path: Pfad zur ChromaDB
    
    Returns:
        Liste der Collection-Namen
    """
    if not os.path.exists(db_path):
        print(f"âŒ Fehler: Der ChromaDB-Pfad '{db_path}' existiert nicht.")
        return []
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collections = client.list_collections()
        return [col.name for col in collections]
    except Exception as e:
        print(f"âŒ Fehler beim Auflisten der Collections: {type(e).__name__}: {e}")
        return []

def get_collection_summary(collection_name: str, db_path: str) -> Optional[CollectionStats]:
    """
    Alias fÃ¼r analyze_collection - fÃ¼r bessere Lesbarkeit.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
    
    Returns:
        CollectionStats-Objekt oder None bei Fehler
    """
    return analyze_collection(collection_name, db_path)

def get_quick_stats(db_path: str) -> Dict[str, Union[int, List[str]]]:
    """
    Schnelle Ãœbersicht ohne detaillierte Analyse.
    
    Args:
        db_path: Pfad zur ChromaDB
    
    Returns:
        Dictionary mit Grundstatistiken
    """
    if not os.path.exists(db_path):
        return {"error": f"Pfad '{db_path}' existiert nicht"}
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collections = client.list_collections()
        
        total_chunks = 0
        collection_names = []
        
        for collection in collections:
            collection_names.append(collection.name)
            total_chunks += collection.count()
        
        return {
            "collection_count": len(collections),
            "collection_names": collection_names,
            "total_chunks": total_chunks,
            "database_path": db_path
        }
        
    except Exception as e:
        return {"error": f"Fehler: {type(e).__name__}: {e}"}

def compare_collections(db_path: str, collection_names: List[str]) -> Optional[Dict]:
    """
    Vergleicht mehrere Collections miteinander.
    
    Args:
        db_path: Pfad zur ChromaDB
        collection_names: Liste der zu vergleichenden Collection-Namen
    
    Returns:
        Dictionary mit Vergleichsdaten
    """
    comparison = {
        "collections": [],
        "summary": {
            "total_chunks": 0,
            "total_documents": 0,
            "analyzed_collections": 0
        }
    }
    
    for name in collection_names:
        stats = analyze_collection(name, db_path)
        if stats:
            comparison["collections"].append(stats.to_dict())
            comparison["summary"]["total_chunks"] += stats.chunk_count
            comparison["summary"]["total_documents"] += stats.document_count
            comparison["summary"]["analyzed_collections"] += 1
    
    if comparison["summary"]["analyzed_collections"] == 0:
        return None
    
    # Durchschnittswerte berechnen
    count = comparison["summary"]["analyzed_collections"]
    comparison["summary"]["avg_chunks_per_collection"] = comparison["summary"]["total_chunks"] / count
    comparison["summary"]["avg_documents_per_collection"] = comparison["summary"]["total_documents"] / count
    
def export_statistics_to_json(db_path: str, output_file: str = "chromadb_stats.json") -> bool:
    """
    Exportiert die Datenbankstatistiken in eine JSON-Datei.
    
    Args:
        db_path: Pfad zur ChromaDB
        output_file: Name der Ausgabedatei
    
    Returns:
        True bei Erfolg, False bei Fehler
    """
    db_stats = get_database_statistics(db_path)
    if not db_stats:
        return False
    
    # Konvertiere zu JSON-serialisierbarem Format
    export_data = {
        "timestamp": datetime.now().isoformat(),
        "database_path": db_path,
        "summary": {
            "total_collections": db_stats.collection_count,
            "total_documents": db_stats.total_documents,
            "total_chunks": db_stats.total_chunks,
            "avg_chunks_per_document": db_stats.get_avg_chunks_per_document()
        },
        "collections": [stats.to_dict() for stats in db_stats.collections]
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… Statistiken erfolgreich exportiert nach: {output_file}")
        return True
    except Exception as e:
        print(f"âŒ Fehler beim Exportieren: {type(e).__name__}: {e}")
        return False

def get_collection_chunks(collection_name: str, db_path: str, limit: int = 10, 
                         offset: int = 0, include_embeddings: bool = False) -> Optional[Dict]:
    """
    Ruft einzelne Chunks einer Collection ab.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
        limit: Maximale Anzahl Chunks (0 = alle)
        offset: Start-Position fÃ¼r Paginierung
        include_embeddings: Ob Embeddings eingeschlossen werden sollen
    
    Returns:
        Dictionary mit Chunk-Daten oder None bei Fehler
    """
    if not os.path.exists(db_path):
        print(f"âŒ Fehler: Der ChromaDB-Pfad '{db_path}' existiert nicht.")
        return None
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(collection_name)
    except Exception as e:
        print(f"âŒ Fehler beim Abrufen der Collection '{collection_name}': {type(e).__name__}: {e}")
        return None
    
    total_chunks = collection.count()
    if total_chunks == 0:
        return {
            "collection_name": collection_name,
            "total_chunks": 0,
            "chunks": [],
            "pagination": {"offset": 0, "limit": limit, "has_more": False}
        }
    
    try:
        # Include-Parameter basierend auf Anforderung
        include_params = ['metadatas', 'documents']
        if include_embeddings:
            include_params.append('embeddings')
        
        # Alle Daten abrufen und dann paginieren (ChromaDB hat keine native Pagination)
        results = collection.get(
            ids=None,
            where=None,
            include=include_params
        )
        
        ids = results.get('ids', [])
        metadatas = results.get('metadatas', [])
        documents = results.get('documents', [])
        embeddings = results.get('embeddings', []) if include_embeddings else []
        
        # Paginierung anwenden
        end_index = offset + limit if limit > 0 else len(ids)
        chunk_slice = slice(offset, end_index)
        
        chunks = []
        for i in range(offset, min(end_index, len(ids))):
            chunk_data = {
                'id': ids[i] if i < len(ids) else None,
                'metadata': metadatas[i] if i < len(metadatas) else {},
                'document': documents[i] if i < len(documents) else '',
                'chunk_size': len(documents[i]) if i < len(documents) and documents[i] else 0,
                'index': i
            }
            
            if include_embeddings and i < len(embeddings):
                chunk_data['embedding'] = embeddings[i]
                chunk_data['embedding_size'] = len(embeddings[i]) if embeddings[i] else 0
            
            chunks.append(chunk_data)
        
        has_more = end_index < len(ids) if limit > 0 else False
        
        return {
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "chunks": chunks,
            "pagination": {
                "offset": offset,
                "limit": limit,
                "returned_count": len(chunks),
                "has_more": has_more
            }
        }
        
    except Exception as e:
        print(f"âŒ Fehler beim Abrufen der Chunks: {type(e).__name__}: {e}")
        return None

def display_collection_chunks(collection_name: str, db_path: str, limit: int = 5, 
                            show_full_text: bool = False, show_metadata: bool = True) -> None:
    """
    Zeigt einzelne Chunks einer Collection formatiert an.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
        limit: Anzahl anzuzeigender Chunks
        show_full_text: Ob vollstÃ¤ndiger Text angezeigt werden soll
        show_metadata: Ob Metadaten angezeigt werden sollen
    """
    chunk_data = get_collection_chunks(collection_name, db_path, limit=limit)
    if not chunk_data:
        return
    
    print(f"\nğŸ§© CHUNKS DER COLLECTION '{collection_name}'")
    print(f"{'='*60}")
    print(f"ğŸ“Š Gesamtanzahl Chunks: {chunk_data['total_chunks']}")
    print(f"ğŸ“‹ Angezeigte Chunks: {chunk_data['pagination']['returned_count']}")
    if chunk_data['pagination']['has_more']:
        print(f"â¡ï¸ Weitere Chunks verfÃ¼gbar")
    
    if not chunk_data['chunks']:
        print("âš ï¸ Keine Chunks gefunden.")
        return
    
    for i, chunk in enumerate(chunk_data['chunks'], 1):
        print(f"\nğŸ—‚ï¸ Chunk {i} (Index: {chunk['index']})")
        print("-" * 40)
        
        # ID anzeigen
        if chunk['id']:
            print(f"ğŸ†” ID: {chunk['id']}")
        
        # Dokument/Text anzeigen
        document = chunk.get('document', '')
        if document:
            if show_full_text:
                print(f"ğŸ“„ Volltext ({chunk['chunk_size']} Zeichen):")
                print(f"   {document}")
            else:
                # Kurze Vorschau (erste 200 Zeichen)
                preview = document[:200] + "..." if len(document) > 200 else document
                print(f"ğŸ“„ Vorschau ({chunk['chunk_size']} Zeichen):")
                print(f"   {preview}")
        else:
            print("ğŸ“„ Kein Dokumententext vorhanden")
        
        # Metadaten anzeigen
        if show_metadata and chunk.get('metadata'):
            print(f"ğŸ·ï¸ Metadaten:")
            for key, value in chunk['metadata'].items():
                # Lange Werte kÃ¼rzen
                if isinstance(value, str) and len(str(value)) > 100:
                    display_value = str(value)[:100] + "..."
                else:
                    display_value = value
                print(f"   â€¢ {key}: {display_value}")
        
        # Embedding-Info (falls vorhanden)
        if 'embedding_size' in chunk:
            print(f"ğŸ§  Embedding-Dimensionen: {chunk['embedding_size']}")

def search_chunks_by_source(collection_name: str, db_path: str, source_filter: str) -> Optional[List[Dict]]:
    """
    Filtert Chunks nach einer bestimmten Quelle.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
        source_filter: Quelldatei zum Filtern
    
    Returns:
        Liste der gefilterten Chunks oder None bei Fehler
    """
    chunk_data = get_collection_chunks(collection_name, db_path, limit=0)  # Alle Chunks
    if not chunk_data:
        return None
    
    filtered_chunks = []
    for chunk in chunk_data['chunks']:
        metadata = chunk.get('metadata', {})
        source = metadata.get('source', '')
        
        if source_filter.lower() in source.lower():
            filtered_chunks.append(chunk)
    
    return filtered_chunks

def get_chunk_by_id(collection_name: str, db_path: str, chunk_id: str) -> Optional[Dict]:
    """
    Ruft einen spezifischen Chunk anhand seiner ID ab.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
        chunk_id: ID des gewÃ¼nschten Chunks
    
    Returns:
        Chunk-Daten oder None bei Fehler/nicht gefunden
    """
    if not os.path.exists(db_path):
        print(f"âŒ Fehler: Der ChromaDB-Pfad '{db_path}' existiert nicht.")
        return None
    
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(collection_name)
        
        results = collection.get(
            ids=[chunk_id],
            include=['metadatas', 'documents']
        )
        
        if not results['ids']:
            return None
        
        return {
            'id': results['ids'][0],
            'metadata': results['metadatas'][0] if results['metadatas'] else {},
            'document': results['documents'][0] if results['documents'] else '',
            'chunk_size': len(results['documents'][0]) if results['documents'] and results['documents'][0] else 0
        }
        
    except Exception as e:
        print(f"âŒ Fehler beim Abrufen des Chunks: {type(e).__name__}: {e}")
        return None

def analyze_chunk_sizes(collection_name: str, db_path: str) -> Optional[Dict]:
    """
    Analysiert die GrÃ¶ÃŸenverteilung der Chunks in einer Collection.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
    
    Returns:
        Dictionary mit GrÃ¶ÃŸenstatistiken
    """
    chunk_data = get_collection_chunks(collection_name, db_path, limit=0)
    if not chunk_data or not chunk_data['chunks']:
        return None
    
    sizes = [chunk['chunk_size'] for chunk in chunk_data['chunks']]
    
    if not sizes:
        return None
    
    sizes.sort()
    n = len(sizes)
    
    # Statistiken berechnen
    stats = {
        'total_chunks': n,
        'min_size': min(sizes),
        'max_size': max(sizes),
        'avg_size': sum(sizes) / n,
        'median_size': sizes[n // 2] if n % 2 == 1 else (sizes[n // 2 - 1] + sizes[n // 2]) / 2,
        'size_distribution': {
            'very_small': len([s for s in sizes if s < 100]),      # < 100 Zeichen
            'small': len([s for s in sizes if 100 <= s < 500]),     # 100-499 Zeichen
            'medium': len([s for s in sizes if 500 <= s < 1500]),   # 500-1499 Zeichen
            'large': len([s for s in sizes if 1500 <= s < 3000]),   # 1500-2999 Zeichen
            'very_large': len([s for s in sizes if s >= 3000])      # >= 3000 Zeichen
        }
    }
    
    return stats

def display_chunk_size_analysis(collection_name: str, db_path: str) -> None:
    """
    Zeigt eine Analyse der Chunk-GrÃ¶ÃŸen formatiert an.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
    """
    stats = analyze_chunk_sizes(collection_name, db_path)
    if not stats:
        print(f"âŒ Konnte Chunk-GrÃ¶ÃŸen fÃ¼r Collection '{collection_name}' nicht analysieren.")
        return
    
    print(f"\nğŸ“ CHUNK-GRÃ–SSEN ANALYSE '{collection_name}'")
    print(f"{'='*50}")
    print(f"ğŸ“Š Gesamtanzahl Chunks: {stats['total_chunks']}")
    print(f"ğŸ“ Minimale GrÃ¶ÃŸe: {stats['min_size']:,} Zeichen")
    print(f"ğŸ“ Maximale GrÃ¶ÃŸe: {stats['max_size']:,} Zeichen")
    print(f"ğŸ“ Durchschnittliche GrÃ¶ÃŸe: {stats['avg_size']:.1f} Zeichen")
    print(f"ğŸ“ Median-GrÃ¶ÃŸe: {stats['median_size']:.1f} Zeichen")
    
    print(f"\nğŸ“Š GrÃ¶ÃŸenverteilung:")
    dist = stats['size_distribution']
    total = stats['total_chunks']
    
    categories = [
        ('Sehr klein (< 100)', dist['very_small']),
        ('Klein (100-499)', dist['small']),
        ('Mittel (500-1499)', dist['medium']),
        ('GroÃŸ (1500-2999)', dist['large']),
        ('Sehr groÃŸ (â‰¥ 3000)', dist['very_large'])
    ]
    
    for category, count in categories:
        percentage = (count / total) * 100 if total > 0 else 0
        bar = "â–ˆ" * int(percentage / 5)  # Einfache Balkenanzeige
        print(f"   {category:<20}: {count:>4} ({percentage:>5.1f}%) {bar}")

def export_chunks_to_json(collection_name: str, db_path: str, output_file: str, 
                         limit: int = 0, include_embeddings: bool = False) -> bool:
    """
    Exportiert Chunks einer Collection in eine JSON-Datei.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
        output_file: Name der Ausgabedatei
        limit: Maximale Anzahl Chunks (0 = alle)
        include_embeddings: Ob Embeddings exportiert werden sollen
    
    Returns:
        True bei Erfolg, False bei Fehler
    """
    chunk_data = get_collection_chunks(collection_name, db_path, limit=limit, 
                                     include_embeddings=include_embeddings)
    if not chunk_data:
        return False
    
    export_data = {
        "timestamp": datetime.now().isoformat(),
        "collection_name": collection_name,
        "database_path": db_path,
        "export_settings": {
            "limit": limit,
            "include_embeddings": include_embeddings
        },
        "summary": {
            "total_chunks_in_collection": chunk_data['total_chunks'],
            "exported_chunks": len(chunk_data['chunks'])
        },
        "chunks": chunk_data['chunks']
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… {len(chunk_data['chunks'])} Chunks erfolgreich exportiert nach: {output_file}")
        return True
    except Exception as e:
        print(f"âŒ Fehler beim Exportieren: {type(e).__name__}: {e}")
        return False
    """
    Exportiert die Datenbankstatistiken in eine JSON-Datei.
    
    Args:
        db_path: Pfad zur ChromaDB
        output_file: Name der Ausgabedatei
    
    Returns:
        True bei Erfolg, False bei Fehler
    """
    db_stats = get_database_statistics(db_path)
    if not db_stats:
        return False
    
    # Konvertiere zu JSON-serialisierbarem Format
    export_data = {
        "timestamp": datetime.now().isoformat(),
        "database_path": db_path,
        "summary": {
            "total_collections": db_stats.collection_count,
            "total_documents": db_stats.total_documents,
            "total_chunks": db_stats.total_chunks,
            "avg_chunks_per_document": db_stats.get_avg_chunks_per_document()
        },
        "collections": [stats.to_dict() for stats in db_stats.collections]
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… Statistiken erfolgreich exportiert nach: {output_file}")
        return True
    except Exception as e:
        print(f"âŒ Fehler beim Exportieren: {type(e).__name__}: {e}")
        return False

def print_collection_summary(collection_name: str, db_path: str) -> None:
    """
    Gibt eine formatierte Zusammenfassung einer Collection aus.
    
    Args:
        collection_name: Name der Collection
        db_path: Pfad zur ChromaDB
    """
    stats = analyze_collection(collection_name, db_path)
    if not stats:
        print(f"âŒ Konnte Collection '{collection_name}' nicht analysieren.")
        return
    
    print(f"\nğŸ“Š Collection: '{stats.name}'")
    print(f"{'='*50}")
    print(f"ğŸ“„ Dokumente (Dateien): {stats.document_count}")
    print(f"ğŸ§© Chunks: {stats.chunk_count}")
    
    if stats.chunk_count > 0:
        print(f"ğŸ“Š Chunks pro Dokument: {stats.get_chunks_per_document():.1f}")
        print(f"ğŸ“ Ã˜ Chunk-GrÃ¶ÃŸe: {stats.avg_chunk_size:.0f} Zeichen")
        
        if stats.source_stats:
            print(f"\nğŸ“ Quelldateien ({len(stats.source_stats)}):")
            sorted_sources = sorted(stats.source_stats.items(), key=lambda x: x[1], reverse=True)
            for source, count in sorted_sources[:5]:  # Top 5 anzeigen
                percentage = (count / stats.chunk_count) * 100
                print(f"   â€¢ {source}: {count} Chunks ({percentage:.1f}%)")
            
            if len(sorted_sources) > 5:
                print(f"   ... und {len(sorted_sources) - 5} weitere")
        
        if stats.metadata_keys:
            print(f"\nğŸ·ï¸ Metadaten-Felder: {', '.join(stats.metadata_keys)}")
    else:
        print("âš ï¸ Collection ist leer")

def display_chromadb_statistics(db_path: str, detailed: bool = True) -> None:
    """
    Zeigt umfassende Statistiken der ChromaDB-Collections an.
    
    Args:
        db_path: Pfad zur ChromaDB
        detailed: Ob detaillierte Statistiken angezeigt werden sollen
    """
    print(f"ğŸ” Verbinde zu ChromaDB unter: {db_path}...")
    
    db_stats = get_database_statistics(db_path)
    if not db_stats:
        return
    
    print("âœ… Verbindung zur ChromaDB erfolgreich.")
    
    if not db_stats.collections:
        print("\nâ„¹ï¸ Keine Collections in dieser ChromaDB gefunden.")
        return
    
    # Ãœbersicht
    print(f"\n{'='*60}")
    print(f"ğŸ“Š CHROMADB STATISTIKEN")
    print(f"{'='*60}")
    print(f"ğŸ“ Anzahl Collections: {db_stats.collection_count}")
    print(f"ğŸ“„ Gesamtanzahl Dokumente (Dateien): {db_stats.total_documents}")
    print(f"ğŸ§© Gesamtanzahl Chunks: {db_stats.total_chunks}")
    if db_stats.total_documents > 0:
        print(f"ğŸ“Š Durchschnittliche Chunks pro Dokument: {db_stats.get_avg_chunks_per_document():.1f}")
    print(f"ğŸ“… Analysiert am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Details pro Collection
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ COLLECTION DETAILS")
    print(f"{'='*60}")
    
    for i, stats in enumerate(db_stats.collections, 1):
        print(f"\nğŸ—‚ï¸ Collection {i}: '{stats.name}'")
        print(f"   ğŸ“„ Anzahl Dokumente (Dateien): {stats.document_count}")
        print(f"   ğŸ§© Anzahl Chunks: {stats.chunk_count}")
        
        if stats.chunk_count == 0:
            print("   âš ï¸ Collection ist leer")
            continue
        
        if stats.document_count > 0:
            print(f"   ğŸ“Š Durchschnittliche Chunks pro Dokument: {stats.get_chunks_per_document():.1f}")
        
        if detailed:
            print(f"   ğŸ“ Durchschnittliche Chunk-GrÃ¶ÃŸe: {stats.avg_chunk_size:.1f} Zeichen")
            
            if stats.metadata_keys:
                print(f"   ğŸ·ï¸ VerfÃ¼gbare Metadaten-Felder: {', '.join(stats.metadata_keys)}")
            
            if stats.source_stats:
                print("   ğŸ“ Chunks pro Quelldatei:")
                for source, count in sorted(stats.source_stats.items()):
                    percentage = (count / stats.chunk_count) * 100
                    print(f"      â€¢ '{source}': {count} Chunks ({percentage:.1f}%)")
            else:
                print("   âš ï¸ Keine Quellinformationen gefunden")
        
        print("-" * 50)

# --- AusfÃ¼hrung ---
if __name__ == "__main__":
    # Standard DB-Pfad - hier anpassen!
    DEFAULT_DB_PATH = "./multimodal_chromadb"
    
    # Benutzer nach DB-Pfad fragen oder Standard verwenden
    db_path = input(f"ChromaDB-Pfad [{DEFAULT_DB_PATH}]: ").strip()
    if not db_path:
        db_path = DEFAULT_DB_PATH
    
    # VerfÃ¼gbare Optionen anzeigen
    print("\n" + "="*60)
    print("ğŸ“Š CHROMADB STATISTICS - HAUPTMENÃœ")
    print("="*60)
    print("1. Schnelle Ãœbersicht")
    print("2. Detaillierte Datenbankanalyse")
    print("3. Spezifische Collection analysieren")
    print("4. Collection-Chunks anzeigen")
    print("5. Chunk-GrÃ¶ÃŸenanalyse")
    print("6. Chunks nach Quelle filtern")
    print("7. Spezifischen Chunk anzeigen (nach ID)")
    print("8. Collections vergleichen")
    print("9. Export: Statistiken als JSON")
    print("10. Export: Chunks als JSON")
    
    choice = input("WÃ¤hlen Sie eine Option (1-10) [2]: ").strip()
    
    if choice == "1":
        # Schnelle Ãœbersicht
        quick = get_quick_stats(db_path)
        if "error" in quick:
            print(f"âŒ {quick['error']}")
        else:
            print(f"\nğŸ“Š Schnelle Ãœbersicht:")
            print(f"Collections: {quick['collection_count']}")
            print(f"Gesamtchunks: {quick['total_chunks']}")
            print(f"Collection-Namen: {', '.join(quick['collection_names'])}")
    
    elif choice == "3":
        # Spezifische Collection
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                print_collection_summary(col_name, db_path)
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    elif choice == "4":
        # Collection-Chunks anzeigen
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                limit = input("Anzahl Chunks anzeigen [5]: ").strip()
                limit = int(limit) if limit.isdigit() else 5
                
                show_full = input("VollstÃ¤ndigen Text anzeigen? (j/n) [n]: ").strip().lower()
                show_full_text = show_full in ['j', 'ja', 'y', 'yes']
                
                display_collection_chunks(col_name, db_path, limit=limit, 
                                        show_full_text=show_full_text)
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    elif choice == "5":
        # Chunk-GrÃ¶ÃŸenanalyse
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                display_chunk_size_analysis(col_name, db_path)
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    elif choice == "6":
        # Chunks nach Quelle filtern
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                source_filter = input("Quelldatei-Filter eingeben: ").strip()
                filtered_chunks = search_chunks_by_source(col_name, db_path, source_filter)
                
                if filtered_chunks:
                    print(f"\nğŸ” Gefilterte Chunks ({len(filtered_chunks)} gefunden):")
                    print("="*50)
                    
                    for i, chunk in enumerate(filtered_chunks[:10], 1):  # Max 10 anzeigen
                        source = chunk.get('metadata', {}).get('source', 'Unbekannt')
                        preview = chunk.get('document', '')[:100] + "..." if len(chunk.get('document', '')) > 100 else chunk.get('document', '')
                        print(f"\n{i}. ID: {chunk.get('id', 'N/A')}")
                        print(f"   ğŸ“ Quelle: {source}")
                        print(f"   ğŸ“„ Vorschau: {preview}")
                    
                    if len(filtered_chunks) > 10:
                        print(f"\n... und {len(filtered_chunks) - 10} weitere Chunks")
                else:
                    print(f"âŒ Keine Chunks mit Quelle '{source_filter}' gefunden.")
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    elif choice == "7":
        # Spezifischen Chunk anzeigen
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                chunk_id = input("Chunk-ID eingeben: ").strip()
                chunk = get_chunk_by_id(col_name, db_path, chunk_id)
                
                if chunk:
                    print(f"\nğŸ—‚ï¸ CHUNK DETAILS")
                    print("="*40)
                    print(f"ğŸ†” ID: {chunk['id']}")
                    print(f"ğŸ“ GrÃ¶ÃŸe: {chunk['chunk_size']} Zeichen")
                    print(f"ğŸ“„ Dokument:")
                    print(f"   {chunk['document']}")
                    
                    if chunk['metadata']:
                        print(f"ğŸ·ï¸ Metadaten:")
                        for key, value in chunk['metadata'].items():
                            print(f"   â€¢ {key}: {value}")
                else:
                    print(f"âŒ Chunk mit ID '{chunk_id}' nicht gefunden.")
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    elif choice == "8":
        # Collections vergleichen
        collections = list_collections(db_path)
        if len(collections) >= 2:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_names = input("Collection-Namen eingeben (durch Komma getrennt): ").strip()
            col_list = [name.strip() for name in col_names.split(',') if name.strip()]
            
            if len(col_list) >= 2:
                comparison = compare_collections(db_path, col_list)
                if comparison:
                    print(f"\nğŸ” COLLECTION-VERGLEICH")
                    print("="*50)
                    print(f"ğŸ“Š Analysierte Collections: {comparison['summary']['analyzed_collections']}")
                    print(f"ğŸ“„ Gesamtdokumente: {comparison['summary']['total_documents']}")
                    print(f"ğŸ§© Gesamtchunks: {comparison['summary']['total_chunks']}")
                    print(f"ğŸ“Š Ã˜ Chunks pro Collection: {comparison['summary']['avg_chunks_per_collection']:.1f}")
                    
                    print(f"\nğŸ“‹ Details:")
                    for col_data in comparison['collections']:
                        print(f"  â€¢ {col_data['name']}: {col_data['document_count']} Docs, {col_data['chunk_count']} Chunks")
                else:
                    print("âŒ Vergleich fehlgeschlagen.")
            else:
                print("âŒ Mindestens 2 Collection-Namen erforderlich.")
        else:
            print("âŒ Mindestens 2 Collections fÃ¼r Vergleich erforderlich.")
    
    elif choice == "9":
        # Export Statistiken
        output_file = input("JSON-Dateiname [chromadb_stats.json]: ").strip()
        if not output_file:
            output_file = "chromadb_stats.json"
        
        success = export_statistics_to_json(db_path, output_file)
        if success:
            print("âœ… Statistiken erfolgreich exportiert.")
    
    elif choice == "10":
        # Export Chunks
        collections = list_collections(db_path)
        if collections:
            print(f"\nğŸ“‹ VerfÃ¼gbare Collections: {', '.join(collections)}")
            col_name = input("Collection-Name eingeben: ").strip()
            if col_name in collections:
                output_file = input(f"JSON-Dateiname [{col_name}_chunks.json]: ").strip()
                if not output_file:
                    output_file = f"{col_name}_chunks.json"
                
                limit = input("Anzahl Chunks exportieren (0 = alle) [0]: ").strip()
                limit = int(limit) if limit.isdigit() else 0
                
                include_emb = input("Embeddings einschlieÃŸen? (j/n) [n]: ").strip().lower()
                include_embeddings = include_emb in ['j', 'ja', 'y', 'yes']
                
                success = export_chunks_to_json(col_name, db_path, output_file, 
                                               limit=limit, include_embeddings=include_embeddings)
                if success:
                    print("âœ… Chunks erfolgreich exportiert.")
            else:
                print(f"âŒ Collection '{col_name}' nicht gefunden.")
        else:
            print("âŒ Keine Collections gefunden.")
    
    else:
        # StandardausfÃ¼hrung mit detaillierten Statistiken
        display_chromadb_statistics(db_path)
        
        # Optional: Export in JSON-Datei
        print(f"\n{'='*60}")
        export_choice = input("MÃ¶chten Sie die Statistiken als JSON exportieren? (j/n): ").lower()
        if export_choice in ['j', 'ja', 'y', 'yes']:
            export_statistics_to_json(db_path)